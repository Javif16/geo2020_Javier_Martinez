THERMAL VARIATION


'''
File that will be used to obtain all the thermal images and extract the desired attributes,
such as emissivity or surface temperature.

It will be able to extract files in formats like GeoTIFF or HDF.

Normalization of images will also take place here, in order for the deep learning algorithms
to always take the same type of data. From 0 to 1, easier for algorithms to process.

Then, the necessary images or attributes will be able to be passed to other files, in order
for the deep learning algorithms established in other files to work with the output produced
in this program.
'''

import numpy as np
import os
import math
import glob
import h5py
import rasterio
from rasterio.transform import from_origin
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split


# ---------- FILES & DATASETS ------------------------------------------------------------------------------------------
folder_path = "C:/Users/txiki/OneDrive/Documents/Studies/MSc_Geomatics/2Y/Thesis/Images/Examples"
file_path = "C:/Users/txiki/OneDrive/Documents/Studies/MSc_Geomatics/2Y/Thesis/Images/Examples/ECO_01-05-2024(00.59).h5"
datasets = ['SDS/LST', 'SDS/Emis1', 'SDS/Emis2', 'SDS/Emis3', 'SDS/Emis4', 'SDS/Emis5', 'SDS/QC']


def explore_h5_group(name, obj):
    # group or dataset
    if isinstance(obj, h5py.Group):
        print(f"ðŸ“‚ Group: {name}")
    elif isinstance(obj, h5py.Dataset):
        print(f"ðŸ“„ Dataset: {name} | Shape: {obj.shape} | Data Type: {obj.dtype}")


with h5py.File(file_path, 'r') as f:
    print(f"ðŸ“– Exploring HDF5 File: {file_path}")
    f.visititems(explore_h5_group)

# ---------- AOI -------------------------------------------------------------------------------------------------------
# area of interest (same area for all images)
SW_lat, SW_lon = 38.76000, -3.88000
NE_lat, NE_lon = 38.77000, -3.87000
# Bbox
# SW_lat, SW_lon = 37.3819416, -7.51104221
# NE_lat, NE_lon = 42.30566877, -1.00161894
check_pixels = [(38.75012, -3.87543), (38.80567, -3.81234)]


def get_aoi(file, sw_lat, sw_lon, ne_lat, ne_lon):
    with h5py.File(file, 'r') as f:
        # bbox from metadata
        north = f['StandardMetadata/NorthBoundingCoordinate'][()]
        south = f['StandardMetadata/SouthBoundingCoordinate'][()]
        east = f['StandardMetadata/EastBoundingCoordinate'][()]
        west = f['StandardMetadata/WestBoundingCoordinate'][()]

        rows, cols = f[datasets[0]].shape

        # lat/lon grids using np.meshgrid to create 2D arrays
        lat_grid, lon_grid = np.meshgrid(np.linspace(south, north, rows), np.linspace(west, east, cols))

        # masking to get pixels from area of interest
        lat_mask = (lat_grid >= sw_lat) & (lat_grid <= ne_lat)
        lon_mask = (lon_grid >= sw_lon) & (lon_grid <= ne_lon)
        valid_rows, valid_cols = np.where(lat_mask)[0], np.where(lon_mask)[1]

        if len(valid_rows) == 0 or len(valid_cols) == 0:
            print(f"Warning: AOI not found in {os.path.basename(file)}")
            return None
        row_min, row_max = max(valid_rows.min(), 0), min(valid_rows.max(), rows - 1)
        col_min, col_max = max(valid_cols.min(), 0), min(valid_cols.max(), cols - 1)
        print(f"AOI found: ({row_min}, {row_max}), ({col_min}, {col_max})")  # Debugging line

        # Slice the lat and lon grids to the AOI
        lat_aoi = lat_grid[row_min:row_max + 1, col_min:col_max + 1]
        lon_aoi = lon_grid[row_min:row_max + 1, col_min:col_max + 1]
        # Print the shapes of the lat/lon AOI grids
        print(f"AOI latitude grid shape: {lat_aoi.shape}")
        print(f"AOI longitude grid shape: {lon_aoi.shape}")
        print(row_min)
        print("---------")
        print(row_max)
        print("---------")
        print(col_min)
        print("---------")
        print(col_max)
        print("---------")
        print(lat_grid)
        print("---------")
        print(lon_grid)
        return row_min, row_max, col_min, col_max, lat_grid, lon_grid

'''
def get_pixel_index(lat, lon, lat_grid, lon_grid):
    row_idx = np.abs(lat_grid - lat).argmin()  # closest row
    col_idx = np.abs(lon_grid - lon).argmin()  # closest column
    return row_idx, col_idx
'''


# ------------------- QC ------------------------------------------------
# checking pixel quality
def check_pixel_quality(qc_value):
    # bits extracted according to ECOSTRESS manual guide
    bit1 = (qc_value >> 0) & 1  # Bit 1
    bit2 = (qc_value >> 1) & 1  # Bit 2

    if bit1 == 0 and bit2 == 0:
        return 1.0  # Best Quality
    elif bit1 == 1 and bit2 == 0:
        return 0.75  # Nominal Quality
    elif bit1 == 0 and bit2 == 1:
        return 0.25  # Cloud Detected
    else:
        return 0.0  # Missing/Bad Data


# Function to extract AOI from multiple emissivity bands and plot them
def aoi_h5_chunk(file, row_min, row_max, col_min, col_max, datasets, chunk_size=512):
    with h5py.File(file, 'r') as f:
        # Read the QC data for the entire dataset
        qc_data = f['SDS/QC'][:]

        # Iterate over each dataset
        for dataset_name in datasets:
            dset = f[dataset_name]
            scale_factor = dset.attrs.get("scale_factor", [1.0])[0]
            add_offset = dset.attrs.get("add_offset", [0.0])[0]
            fill_value = dset.attrs.get("_FillValue", [None])[0]

            # Iterate over the dataset in chunks
            for start_row in range(0, dset.shape[0], chunk_size):
                end_row = min(start_row + chunk_size, dset.shape[0])
                for start_col in range(0, dset.shape[1], chunk_size):
                    end_col = min(start_col + chunk_size, dset.shape[1])

                    # Extract the chunk and apply scale factor and offset
                    chunk = dset[start_row:end_row, start_col:end_col].astype(np.float32) * scale_factor + add_offset

                    # Apply fill value
                    if fill_value is not None:
                        chunk[chunk == fill_value] = np.nan

                    # Convert temperature from Kelvin to Celsius if it's a Land Surface Temperature (LST) dataset
                    if 'LST' in dataset_name:
                        chunk -= 273.15

                    # Determine the corresponding region in the QC data
                    qc_chunk = qc_data[start_row:end_row, start_col:end_col]

                    # Create the cloud mask for the current chunk
                    cloud_mask_chunk = ((qc_chunk >> 0) & 1 == 0) & ((qc_chunk >> 1) & 1 == 1)

                    # Ensure the cloud mask matches the chunk's shape
                    cloud_mask_resized = cloud_mask_chunk[
                                         (row_min - start_row):(row_max - start_row + 1),
                                         (col_min - start_col):(col_max - start_col + 1)
                                         ]

                    # Apply the cloud mask to the chunk
                    if cloud_mask_resized.size > 0:
                        chunk[cloud_mask_resized] = np.nan
                    else:
                        print(f"Warning: Empty cloud mask for chunk starting at row {start_row}, col {start_col}")

                    # Yield the processed chunk
                    yield chunk


# ------------------- DISPLAY ----------------------------------------
def display(images):
    fig, axes = plt.subplots(1, len(datasets), figsize=(8 * len(datasets), 8))
    if len(datasets) == 1:
        axes = [axes]

    for ax, (dataset_name, data_list) in zip(axes, images.items()):
        if data_list:
            avg_data = np.nanmean(np.stack(data_list), axis=0)
            im = ax.imshow(avg_data, cmap='gray', extent=[SW_lon, NE_lon, SW_lat, NE_lat])
            ax.set_title(dataset_name, fontsize=14)
            ax.set_xlabel("Longitude", fontsize=12)
            ax.set_ylabel("Latitude", fontsize=12)
            fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.05, pad=0.06)
        plt.tight_layout()
        plt.show()
        '''
                if 'Emis' in dataset_name or 'LST' in dataset_name:
                    print(f"\n{dataset_name}:")
                    for lat, lon in check_pixels:
                        row_idx, col_idx = get_pixel_index(lat, lon, lat_grid, lon_grid)
                        if row_min <= row_idx <= row_max and col_min <= col_idx <= col_max:
                            value = data[row_idx, col_idx]
                            unit = "Â°C" if 'LST' in dataset_name else ""
                            print(f"  - ({lat}, {lon}): {value:.2f} {unit}")

                            # quality display
                            if qc_data is not None:
                                qc_value = qc_data[row_idx, col_idx]
                                quality = check_pixel_quality(qc_value)
                                print(f"    -> QC Flag: {qc_value} ({quality})")
                            else:
                                print("    -> No QC data available")

                        else:
                            print(f"  - ({lat}, {lon}): Out of AOI bounds")

            plt.tight_layout()
            plt.show()
'''
# extract_and_plot_aoi(folder_path)


# -------------- NORMALIZATION -----------------------------------------------------------------------------------------
# Function to normalize values to range [0, 1]
def normalize(data):
    min_val = np.min(data)
    max_val = np.max(data)
    return ((data - min_val) / (max_val - min_val + 1e-8)).astype(np.float32)


# -------------- PRE-PROCESSING ----------------------------------------------------------------------------------------
'''
# Images  into Numpy arrays
def arrays(images):
    data = []
    for i in range(len(next(iter(images.values())))):
        stacked_images = []
        for dataset_images in images.values():
            stacked_images.append(normalize(dataset_images[i]))
        stacked_images = np.stack(stacked_images, axis=-1)
        data.append(stacked_images)
        # turns all images into a single numpy array

    return np.array(data)  # returning (nÂº samples, 256, 256, nÂº channels)
'''
'''
Shape after arrays:
Â· 1 image - (256, 256, 1)
Â· 10 images - (256, 256, 10)
Â· 150 images - (256, 256, 150)
'''


# -------------- PATCHING ----------------------------------------------------------------------------------------------
def patching(images, patch_size=256, stride=256, label=None):     # stride=128 means 50% overlapping
    """
    Extracts non-overlapping patches from the h5 images.
    :param image: 2D NumPy array (grayscale) or 3D NumPy array (multi-channel)
    :param patch_size: Size of patches
    :param stride: same as patch size to ensure no overlapping
    :return: List of patches
    """
    patches = []
    h, w, c = images.shape  # height and width

    for i in range(0, h - patch_size + 1, stride):  # height
        for j in range(0, w - patch_size + 1, stride):  # width
            patch = images[i:i+patch_size, j:j+patch_size, :]
            patches.append(patch)

    return np.array(patches)

'''
Shape after patching:
Â· 1 image - (nÂº patches, 256, 256, 1)
Â· 10 images - (nÂº patches, 256, 256, 10)
Â· 150 images - (nÂº patches, 256, 256, 150)
'''


# -------------- SPLITS ------------------------------------------------------------------------------------------------
def splitting(patches, labels, test_size=0.1, val_size=0.2, random_state=42):
    # Split data into training (80%), validation (10%), and test (10%)
    # First into training and temporal data (validation + testing)
    X_train, X_temp, y_train, y_temp = train_test_split(patches, labels, test_size=test_size, random_state=random_state)
    # Then into validation and testing
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size/(1-test_size), random_state=random_state)

    return X_train, X_val, X_test, y_train, y_val, y_test


# -------------- GENERATOR & MEMMAP ------------------------------------------------------------------------------------
def data_generator(folder_path, datasets, sw_lat, sw_lon, ne_lat, ne_lon):
    files = glob.glob(os.path.join(folder_path, "*.h5"))
    for file in files:
        indices = get_aoi(file, sw_lat, sw_lon, ne_lat, ne_lon)
        if indices is None:
            continue
        row_min, row_max, col_min, col_max, _, _ = indices
        print(f"File: {file}, Row range: {row_min}-{row_max}, Col range: {col_min}-{col_max}")

        for img_chunk in aoi_h5_chunk(file, row_min, row_max, col_min, col_max, datasets):
            yield img_chunk

def create_memmap(file_name, shape, dtype=np.float32):
    return np.memmap(file_name, dtype=dtype, mode='w+', shape=shape)

# -------------- WORKFLOW ----------------------------------------------------------------------------------------------
memmap_file = "thermal_images.dat"
mmap_shape = (10000, 256, 256, len(datasets))
mmap_data = create_memmap(memmap_file, mmap_shape)

gen = data_generator(folder_path, datasets, SW_lat, SW_lon, NE_lat, NE_lon)
patches_list = []
labels_list = []

for i, (img_chunk) in enumerate(gen):
    if i >= mmap_shape[0]:
        break
    mmap_data[i, :, :, :] = normalize(img_chunk)
    patches_list.append(patching(img_chunk))
    labels_list.append(np.zeros((patches_list[-1].shape[0], 256, 256, 1)))

mmap_data.flush()
print("Memmap saved successfully!")


# aoi_images, pixel_weights = aoi_h5_chunk(folder_path)
# display(aoi_images)
# stacked = arrays(aoi_images)
# print(stacked.shape)  # ensure num_channels == 6 (1 LST + 5 Emissivity bands)
# print(stacked[..., 0])  # ensure LST is expected data
# print(stacked[..., 1:])  # ensure Emis is expected data
# patches = np.concatenate([patching(img) for img in stacked], axis=0)
# weight_patches = np.concatenate([patching(img) for img in pixel_weights], axis=0)
# print(f"Total patches from all images: {patches.shape}")

patches = np.concatenate(patches_list, axis=0)
labels = np.concatenate(labels_list, axis=0)

#labels = np.zeros((patches.shape[0], 256, 256, 1))  # change when I have labels ready
X_train, X_val, X_test, y_train, y_val, y_test = splitting(patches, labels)
# checks to make sure CNN receives correct input shape
print(X_train.shape, X_val.shape, X_test.shape)  # expected (N, 256, 256, nÂº channels)
print(y_train.shape, y_val.shape, y_test.shape)  # expected (N, 256, 256, 1)
# in case shape is not correct, use below
'''
X_train = np.expand_dims(X_train, axis=-1)  # Makes it (N, 256, 256, 1)
X_val = np.expand_dims(X_val, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)
'''

np.save("X_train.npy", X_train.astype(np.float32))
np.save("X_val.npy", X_val.astype(np.float32))
np.save("X_test.npy", X_test.astype(np.float32))
np.save("y_train.npy", y_train.astype(np.float32))
np.save("y_val.npy", y_val.astype(np.float32))
np.save("y_test.npy", y_test.astype(np.float32))


print("âœ… Data saved successfully!")

'''Convert to TensorFlow dataset
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32).shuffle(len(X_train)).prefetch(tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32).prefetch(tf.data.AUTOTUNE)
test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32).prefetch(tf.data.AUTOTUNE)'''
# Return theses splits to be used as input in models