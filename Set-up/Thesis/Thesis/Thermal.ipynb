{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11043acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cefa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Exploring HDF5 File: C:/Users/txiki/OneDrive/Documents/Studies/MSc_Geomatics/2Y/Thesis/Images/Examples/ECO_01-05-2024(00.59).h5\n",
      "ðŸ“‚ Group: L2 LSTE Metadata\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/AncillaryNWP | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/BandSpecification | Shape: (6,) | Data Type: float32\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/CloudMaxTemperature | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/CloudMeanTemperature | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/CloudMinTemperature | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/CloudSDevTemperature | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/Emis1GoodAvg | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/Emis2GoodAvg | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/Emis3GoodAvg | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/Emis4GoodAvg | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/Emis5GoodAvg | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/LSTGoodAvg | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/NWPSource | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/NumberOfBands | Shape: (1,) | Data Type: uint8\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/OrbitCorrectionPerformed | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/QAPercentCloudCover | Shape: (1,) | Data Type: int32\n",
      "ðŸ“„ Dataset: L2 LSTE Metadata/QAPercentGoodQuality | Shape: (1,) | Data Type: int32\n",
      "ðŸ“‚ Group: SDS\n",
      "ðŸ“„ Dataset: SDS/Emis1 | Shape: (5632, 5400) | Data Type: uint8\n",
      "ðŸ“„ Dataset: SDS/Emis1_err | Shape: (5632, 5400) | Data Type: uint16\n",
      "ðŸ“„ Dataset: SDS/Emis2 | Shape: (5632, 5400) | Data Type: uint8\n",
      "ðŸ“„ Dataset: SDS/Emis2_err | Shape: (5632, 5400) | Data Type: uint16\n",
      "ðŸ“„ Dataset: SDS/Emis3 | Shape: (5632, 5400) | Data Type: uint8\n",
      "ðŸ“„ Dataset: SDS/Emis3_err | Shape: (5632, 5400) | Data Type: uint16\n",
      "ðŸ“„ Dataset: SDS/Emis4 | Shape: (5632, 5400) | Data Type: uint8\n",
      "ðŸ“„ Dataset: SDS/Emis4_err | Shape: (5632, 5400) | Data Type: uint16\n",
      "ðŸ“„ Dataset: SDS/Emis5 | Shape: (5632, 5400) | Data Type: uint8\n",
      "ðŸ“„ Dataset: SDS/Emis5_err | Shape: (5632, 5400) | Data Type: uint16\n",
      "ðŸ“„ Dataset: SDS/EmisWB | Shape: (5632, 5400) | Data Type: uint8\n",
      "ðŸ“„ Dataset: SDS/LST | Shape: (5632, 5400) | Data Type: uint16\n",
      "ðŸ“„ Dataset: SDS/LST_err | Shape: (5632, 5400) | Data Type: uint8\n",
      "ðŸ“„ Dataset: SDS/PWV | Shape: (5632, 5400) | Data Type: uint16\n",
      "ðŸ“„ Dataset: SDS/QC | Shape: (5632, 5400) | Data Type: uint16\n",
      "ðŸ“‚ Group: StandardMetadata\n",
      "ðŸ“„ Dataset: StandardMetadata/AncillaryInputPointer | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/AutomaticQualityFlag | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/AutomaticQualityFlagExplanation | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/BuildID | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/CampaignShortName | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/CollectionLabel | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/DataFormatType | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/DayNightFlag | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/EastBoundingCoordinate | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: StandardMetadata/HDFVersionID | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ImageLineSpacing | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: StandardMetadata/ImageLines | Shape: (1,) | Data Type: int32\n",
      "ðŸ“„ Dataset: StandardMetadata/ImagePixelSpacing | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: StandardMetadata/ImagePixels | Shape: (1,) | Data Type: int32\n",
      "ðŸ“„ Dataset: StandardMetadata/InputPointer | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/InstrumentShortName | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/LocalGranuleID | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/LongName | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/NorthBoundingCoordinate | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: StandardMetadata/PGEName | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/PGEVersion | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/PlatformLongName | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/PlatformShortName | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/PlatformType | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ProcessingEnvironment | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ProcessingLevelDescription | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ProcessingLevelID | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ProducerAgency | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ProducerInstitution | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ProductionDateTime | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ProductionLocation | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/RangeBeginningDate | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/RangeBeginningTime | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/RangeEndingDate | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/RangeEndingTime | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/RegionID | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/SISName | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/SISVersion | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/SceneID | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/ShortName | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/SouthBoundingCoordinate | Shape: (1,) | Data Type: float64\n",
      "ðŸ“„ Dataset: StandardMetadata/StartOrbitNumber | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/StopOrbitNumber | Shape: () | Data Type: object\n",
      "ðŸ“„ Dataset: StandardMetadata/WestBoundingCoordinate | Shape: (1,) | Data Type: float64\n"
     ]
    }
   ],
   "source": [
    "# ---------- FILES & DATASETS ------------------------------------------------------------------------------------------\n",
    "folder_path = \"C:/Users/txiki/OneDrive/Documents/Studies/MSc_Geomatics/2Y/Thesis/Images/Examples\"\n",
    "file_path = \"C:/Users/txiki/OneDrive/Documents/Studies/MSc_Geomatics/2Y/Thesis/Images/Examples/ECO_01-05-2024(00.59).h5\"\n",
    "datasets = ['SDS/LST', 'SDS/Emis1', 'SDS/Emis2', 'SDS/Emis3', 'SDS/Emis4', 'SDS/Emis5', 'SDS/QC']\n",
    "\n",
    "def explore_h5_group(name, obj):\n",
    "    # group or dataset\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        print(f\"ðŸ“‚ Group: {name}\")\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "        print(f\"ðŸ“„ Dataset: {name} | Shape: {obj.shape} | Data Type: {obj.dtype}\")\n",
    "\n",
    "\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    print(f\"ðŸ“– Exploring HDF5 File: {file_path}\")\n",
    "    f.visititems(explore_h5_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1f741c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_pixel_index(lat, lon, lat_grid, lon_grid):\\n    row_idx = np.abs(lat_grid - lat).argmin()  # closest row\\n    col_idx = np.abs(lon_grid - lon).argmin()  # closest column\\n    return row_idx, col_idx\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- AOI -------------------------------------------------------------------------------------------------------\n",
    "# area of interest (same area for all images)\n",
    "SW_lat, SW_lon = 38.69758, -3.91992\n",
    "NE_lat, NE_lon = 38.85585, -3.72656\n",
    "check_pixels = [(38.75012, -3.87543), (38.80567, -3.81234)]\n",
    "\n",
    "\n",
    "def get_aoi(file, sw_lat, sw_lon, ne_lat, ne_lon):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        # bbox from metadata\n",
    "        north = f['StandardMetadata/NorthBoundingCoordinate'][()]\n",
    "        south = f['StandardMetadata/SouthBoundingCoordinate'][()]\n",
    "        east = f['StandardMetadata/EastBoundingCoordinate'][()]\n",
    "        west = f['StandardMetadata/WestBoundingCoordinate'][()]\n",
    "\n",
    "        rows, cols = f[datasets[0]].shape\n",
    "\n",
    "        # lat/lon grids\n",
    "        lat_grid = np.linspace(north, south, rows)[:, None]  # column\n",
    "        lon_grid = np.linspace(west, east, cols)[None, :]  # row vector\n",
    "\n",
    "        # masking to get pixels from area of interest\n",
    "        lat_mask = (lat_grid >= sw_lat) & (lat_grid <= ne_lat)\n",
    "        lon_mask = (lon_grid >= sw_lon) & (lon_grid <= ne_lon)\n",
    "        valid_rows, valid_cols = np.where(lat_mask)[0], np.where(lon_mask)[1]\n",
    "\n",
    "        if len(valid_rows) == 0 or len(valid_cols) == 0:\n",
    "            print(f\"Warning: AOI not found in {os.path.basename(file)}\")\n",
    "            return None\n",
    "        return valid_rows.min(), valid_rows.max(), valid_cols.min(), valid_cols.max(), lat_grid, lon_grid\n",
    "\n",
    "'''\n",
    "def get_pixel_index(lat, lon, lat_grid, lon_grid):\n",
    "    row_idx = np.abs(lat_grid - lat).argmin()  # closest row\n",
    "    col_idx = np.abs(lon_grid - lon).argmin()  # closest column\n",
    "    return row_idx, col_idx\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d05672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- CLOUDS and QC ------------------------------------------------------------------------------------\n",
    "# checking pixel quality\n",
    "def check_pixel_quality(qc_value):\n",
    "    # bits extracted according to ECOSTRESS manual guide\n",
    "    bit1 = (qc_value >> 0) & 1  # Bit 1\n",
    "    bit2 = (qc_value >> 1) & 1  # Bit 2\n",
    "\n",
    "    if bit1 == 0 and bit2 == 0:\n",
    "        return 1.0  # Best Quality\n",
    "    elif bit1 == 1 and bit2 == 0:\n",
    "        return 0.75  # Nominal Quality\n",
    "    elif bit1 == 0 and bit2 == 1:\n",
    "        return 0.25  # Cloud Detected\n",
    "    else:\n",
    "        return 0.0  # Missing/Bad Data\n",
    "\n",
    "def aoi_cloud_mask(folder_path):\n",
    "    files = glob.glob(os.path.join(folder_path, \"*.h5\"))\n",
    "    images = {dataset: [] for dataset in datasets}\n",
    "    weights = {dataset: [] for dataset in datasets}\n",
    "    for file in files:\n",
    "        indices = get_aoi(file, SW_lat, SW_lon, NE_lat, NE_lon)\n",
    "        if indices is None:\n",
    "            continue\n",
    "        row_min, row_max, col_min, col_max, lat_grid, lon_grid = indices\n",
    "\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            # QC for quality checking\n",
    "            qc_data = f[\"SDS/QC\"][:]\n",
    "            # aoi from QC\n",
    "            qc_aoi = qc_data[row_min:row_max + 1, col_min:col_max + 1]\n",
    "\n",
    "            # cloud mask for pixels detected as clouds (10)\n",
    "            cloud_mask = ((qc_aoi >> 0) & 1 == 0) & ((qc_aoi >> 1) & 1 == 1)\n",
    "\n",
    "            pixel_weights = np.vectorize(check_pixel_quality)(qc_aoi)\n",
    "\n",
    "            for dataset_name in datasets:\n",
    "                data = f[dataset_name][:].astype(float)\n",
    "\n",
    "                scale_factor = f[dataset_name].attrs.get(\"scale_factor\", [1.0])[0]\n",
    "                add_offset = f[dataset_name].attrs.get(\"add_offset\", [0.0])[0]\n",
    "                fill_value = f[dataset_name].attrs.get(\"_FillValue\", [None])[0]\n",
    "\n",
    "                # scaling + conditions\n",
    "                data = data * scale_factor + add_offset\n",
    "                # invalid pixels\n",
    "                if fill_value is not None:\n",
    "                    data[data == fill_value] = np.nan\n",
    "                if 'LST' in dataset_name:\n",
    "                    data -= 273.15  # K to ÂºC\n",
    "\n",
    "                # crop the area of interest\n",
    "                cropped_data = data[row_min:row_max + 1, col_min:col_max + 1]\n",
    "                cropped_data[cloud_mask] = np.nan  # remove clouds but keep pixels as nan\n",
    "\n",
    "                weight_data = pixel_weights.copy()\n",
    "                weight_data[np.isnan(cropped_data)] = 0.25\n",
    "\n",
    "                images[dataset_name].append(cropped_data)\n",
    "                weights[dataset_name].append(weight_data)\n",
    "\n",
    "    return images, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33fb71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- DISPLAY ----------------------------------------------------------------------------------\n",
    "def display(images):\n",
    "    fig, axes = plt.subplots(1, len(datasets), figsize=(8 * len(datasets), 8))\n",
    "    if len(datasets) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (dataset_name, data_list) in zip(axes, images.items()):\n",
    "        if data_list:\n",
    "            avg_data = np.nanmean(np.stack(data_list), axis=0)\n",
    "            im = ax.imshow(avg_data, cmap='gray', extent=[SW_lon, NE_lon, SW_lat, NE_lat])\n",
    "            ax.set_title(dataset_name, fontsize=14)\n",
    "            ax.set_xlabel(\"Longitude\", fontsize=12)\n",
    "            ax.set_ylabel(\"Latitude\", fontsize=12)\n",
    "            fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.05, pad=0.06)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        '''\n",
    "                if 'Emis' in dataset_name or 'LST' in dataset_name:\n",
    "                    print(f\"\\n{dataset_name}:\")\n",
    "                    for lat, lon in check_pixels:\n",
    "                        row_idx, col_idx = get_pixel_index(lat, lon, lat_grid, lon_grid)\n",
    "                        if row_min <= row_idx <= row_max and col_min <= col_idx <= col_max:\n",
    "                            value = data[row_idx, col_idx]\n",
    "                            unit = \"Â°C\" if 'LST' in dataset_name else \"\"\n",
    "                            print(f\"  - ({lat}, {lon}): {value:.2f} {unit}\")\n",
    "\n",
    "                            # quality display\n",
    "                            if qc_data is not None:\n",
    "                                qc_value = qc_data[row_idx, col_idx]\n",
    "                                quality = check_pixel_quality(qc_value)\n",
    "                                print(f\"    -> QC Flag: {qc_value} ({quality})\")\n",
    "                            else:\n",
    "                                print(\"    -> No QC data available\")\n",
    "\n",
    "                        else:\n",
    "                            print(f\"  - ({lat}, {lon}): Out of AOI bounds\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "'''\n",
    "# extract_and_plot_aoi(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05cf1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- NORMALIZATION -----------------------------------------------------------------------------------------\n",
    "# values to range [0, 1]\n",
    "def normalize(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    return ((data - min_val) / (max_val - min_val + 1e-8)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3419f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape after arrays:\\nÂ· 1 image - (256, 256, 1)\\nÂ· 10 images - (256, 256, 10)\\nÂ· 150 images - (256, 256, 150)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- PRE-PROCESSING ----------------------------------------------------------------------------------------\n",
    "\n",
    "# Images  into Numpy arrays\n",
    "def arrays(images):\n",
    "    data = []\n",
    "    for i in range(len(next(iter(images.values())))):\n",
    "        stacked_images = []\n",
    "        for dataset_images in images.values():\n",
    "            stacked_images.append(normalize(dataset_images[i]))\n",
    "        stacked_images = np.stack(stacked_images, axis=-1)\n",
    "        data.append(stacked_images)\n",
    "        # turns all images into a single numpy array\n",
    "\n",
    "    return np.array(data)  # returning (nÂº samples, 256, 256, nÂº channels)\n",
    "\n",
    "\n",
    "'''Shape after arrays:\n",
    "Â· 1 image - (256, 256, 1)\n",
    "Â· 10 images - (256, 256, 10)\n",
    "Â· 150 images - (256, 256, 150)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae572873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShape after patching:\\nÂ· 1 image - (nÂº patches, 256, 256, 1)\\nÂ· 10 images - (nÂº patches, 256, 256, 10)\\nÂ· 150 images - (nÂº patches, 256, 256, 150)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- PATCHING ----------------------------------------------------------------------------------------------\n",
    "def patching(images, patch_size=256, stride=256, label=None):     # stride=128 means 50% overlapping\n",
    "    \"\"\"\n",
    "    Extracts non-overlapping patches from the h5 images.\n",
    "    :param image: 2D NumPy array (grayscale) or 3D NumPy array (multi-channel)\n",
    "    :param patch_size: Size of patches\n",
    "    :param stride: same as patch size to ensure no overlapping\n",
    "    :return: List of patches\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    h, w, c = images.shape  # height and width\n",
    "\n",
    "    for i in range(0, h - patch_size + 1, stride):  # height\n",
    "        for j in range(0, w - patch_size + 1, stride):  # width\n",
    "            patch = images[i:i+patch_size, j:j+patch_size, :]\n",
    "            patches.append(patch)\n",
    "\n",
    "    return np.array(patches)\n",
    "\n",
    "'''\n",
    "Shape after patching:\n",
    "Â· 1 image - (nÂº patches, 256, 256, 1)\n",
    "Â· 10 images - (nÂº patches, 256, 256, 10)\n",
    "Â· 150 images - (nÂº patches, 256, 256, 150)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e20eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- SPLITS ------------------------------------------------------------------------------------------------\n",
    "def splitting(patches, labels, test_size=0.1, val_size=0.2, random_state=42):\n",
    "    # Split data into training (80%), validation (10%), and test (10%)\n",
    "    # First into training and temporal data (validation + testing)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(patches, labels, test_size=test_size, random_state=random_state)\n",
    "    # Then into validation and testing\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size/(1-test_size), random_state=random_state)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38cea467",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 232. MiB for an array with shape (5632, 5400) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -------------- WORKFLOW ----------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m aoi_images, pixel_weights \u001b[38;5;241m=\u001b[39m aoi_cloud_mask(folder_path)\n\u001b[0;32m      4\u001b[0m display(aoi_images)\n\u001b[0;32m      6\u001b[0m stacked \u001b[38;5;241m=\u001b[39m arrays(aoi_images)\n",
      "Cell \u001b[1;32mIn[7], line 46\u001b[0m, in \u001b[0;36maoi_cloud_mask\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     43\u001b[0m fill_value \u001b[38;5;241m=\u001b[39m f[dataset_name]\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_FillValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28;01mNone\u001b[39;00m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# scaling + conditions\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;241m*\u001b[39m scale_factor \u001b[38;5;241m+\u001b[39m add_offset\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# invalid pixels\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 232. MiB for an array with shape (5632, 5400) and data type float64"
     ]
    }
   ],
   "source": [
    "# -------------- WORKFLOW ----------------------------------------------------------------------------------------------\n",
    "\n",
    "aoi_images, pixel_weights = aoi_cloud_mask(folder_path)\n",
    "display(aoi_images)\n",
    "\n",
    "stacked = arrays(aoi_images)\n",
    "print(stacked.shape)  # ensure num_channels == 6 (1 LST + 5 Emissivity bands)\n",
    "print(stacked[..., 0])  # ensure LST is expected data\n",
    "print(stacked[..., 1:])  # ensure Emis is expected data\n",
    "patches = np.concatenate([patching(img) for img in stacked], axis=0)\n",
    "weight_patches = np.concatenate([patching(img) for img in pixel_weights], axis=0)\n",
    "print(f\"Total patches from all images: {patches.shape}\")\n",
    "\n",
    "labels = np.zeros((patches.shape[0], 256, 256, 1))  # change when I have labels ready\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splitting(patches, labels)\n",
    "# checks to make sure CNN receives correct input shape\n",
    "print(X_train.shape, X_val.shape, X_test.shape)  # expected (N, 256, 256, nÂº channels)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)  # expected (N, 256, 256, 1)\n",
    "# in case shape is not correct, use below\n",
    "'''\n",
    "X_train = np.expand_dims(X_train, axis=-1)  # Makes it (N, 256, 256, 1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "'''\n",
    "\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "np.save(\"pixel_weights.npy\", weight_patches)\n",
    "\n",
    "\n",
    "print(\"âœ… Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfb98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
