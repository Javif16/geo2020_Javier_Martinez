QOPTUNA


def create_optimizer(trial):
    # We optimize the choice of optimizers as well as their parameters.
    kwargs = {}
    optimizer_options = ["RMSprop", "Adam", "SGD"]
    optimizer_selected = trial.suggest_categorical("optimizer", optimizer_options)
    if optimizer_selected == "RMSprop":
        kwargs["learning_rate"] = trial.suggest_float(
            "rmsprop_learning_rate", 1e-5, 1e-1, log=True
        )
        kwargs["weight_decay"] = trial.suggest_float("rmsprop_weight_decay", 0.85, 0.99)
        kwargs["momentum"] = trial.suggest_float("rmsprop_momentum", 1e-5, 1e-1, log=True)
    elif optimizer_selected == "Adam":
        kwargs["learning_rate"] = trial.suggest_float("adam_learning_rate", 1e-5, 1e-1, log=True)
    elif optimizer_selected == "SGD":
        kwargs["learning_rate"] = trial.suggest_float(
            "sgd_opt_learning_rate", 1e-5, 1e-1, log=True
        )
        kwargs["momentum"] = trial.suggest_float("sgd_opt_momentum", 1e-5, 1e-1, log=True)

    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)
    return optimizer


def learn(model, optimizer, dataset, mode="eval"):
    accuracy = tf.metrics.Accuracy("accuracy", dtype=tf.float32)

    for batch, (images, labels) in enumerate(dataset):
        with tf.GradientTape() as tape:
            logits = model(images, training=(mode == "train"))
            loss_value = tf.reduce_mean(
                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)
            )
            if mode == "eval":
                accuracy(
                    tf.argmax(logits, axis=1, output_type=tf.int64), tf.cast(labels, tf.int64)
                )
            else:
                grads = tape.gradient(loss_value, model.variables)
                optimizer.apply_gradients(zip(grads, model.variables))

    if mode == "eval":
        return accuracy


# FYI: Objective functions can take additional arguments
# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).
def objective(trial):
    # Get MNIST data.
    train_ds, valid_ds = get_mnist()

    # Build model and optimizer.
    model = create_model(trial)
    optimizer = create_optimizer(trial)

    # Training and validating cycle.
    with tf.device("/cpu:0"):
        for _ in range(EPOCHS):
            learn(model, optimizer, train_ds, "train")

        accuracy = learn(model, optimizer, valid_ds, "eval")

    # Return last validation accuracy.
    return accuracy.result()


----------------------------------------------------------- MY CODE -------------------------------------------

import optuna 


# Optuna Objective Function
def objective(trial):
    filters = trial.suggest_categorical("filters", [16, 32, 64])
    dropout = trial.suggest_float("dropout", 0.1, 0.5)
    l2_factor = trial.suggest_float("l2_factor", 1e-5, 1e-3)
    learning_rate = trial.suggest_loguniform("learning_rate", 1e-4, 1e-2)
    optimizer_name = trial.suggest_categorical("optimizer", ["adam", "sgd"])

    # Create Model
    model = unet_model((128, 128, 3), filters, dropout, l2_factor)

    # Optimizer Selection
    if optimizer_name == "adam":
        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    else:
        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)

    model.compile(optimizer=optimizer, loss="binary_crossentropy", metrics=["accuracy"])

    # Train Model
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), 
                        epochs=5, batch_size=16, verbose=0)  # Train for 5 epochs

    # Return Validation Accuracy
    val_acc = history.history["val_accuracy"][-1]
    return val_acc

# Run Optuna Optimization
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)

# Get Best Parameters
best_params = study.best_params
print("Best Parameters:", best_params)

# Train Final Model with Best Hyperparameters
best_model = unet_model((128, 128, 3), best_params["filters"], best_params["dropout"], best_params["l2_factor"])
best_optimizer = keras.optimizers.Adam(learning_rate=best_params["learning_rate"]) if best_params["optimizer"] == "adam" else keras.optimizers.SGD(learning_rate=best_params["learning_rate"], momentum=0.9)

best_model.compile(optimizer=best_optimizer, loss="binary_crossentropy", metrics=["accuracy"])
best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=16)

# Save Best Model
best_model.save("best_unet_model.h5")

optuna.visualization.plot_optimization_history(study).show()
optuna.visualization.plot_param_importances(study).show()


Automates hyperparameter tuning instead of relying in manual guessing
Runs efficient search strategies
Can integrate with K-fold Cross-Validation
Finds best learning rates, dropout values and optimizer choices